{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berkeley CS 285 - Deep Reinforcement Learning, Decision Making, and Control - Fall 2020\n",
    "\n",
    "# Assignment 1: Imitation Learning\n",
    "\n",
    "##### All pictures and slides are from Sergei Levine's course CS285 - Deep RL\n",
    "\n",
    "> http://rail.eecs.berkeley.edu/deeprlcourse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The goal of this assignment is to experiment with imitation learning, including direct behavior cloning and\n",
    "the DAgger algorithm. In lieu of a human demonstrator, demonstrations will be provided via an expert policy\n",
    "that we have trained for you. Your goals will be to set up behavior cloning and DAgger, and compare their\n",
    "performance on a few different continuous control tasks from the OpenAI Gym benchmark suite. Turn in your\n",
    "report and code as described in Section 4.*\n",
    "\n",
    "*The starter-code for this assignment can be found at*\n",
    "\n",
    "> https://github.com/berkeleydeeprlcourse/homework_fall2020\n",
    "\n",
    "*You have the option of running the code either on Google Colab or on your own machine. Please refer to the\n",
    "README for more information on setup.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide screen notebook\n",
    "\n",
    "Do not run if you prefer the default setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "   * Estructura del programa/sistemita\n",
    "   * Solucion de cada parte que habia que solucionar\n",
    "   * Analisis que pÃ­de el ejercicio\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "\n",
    "# Supervised Learning of Behaviours\n",
    "\n",
    "We are going to use the same Supervised Learning setup as a neural network learning to map inputs to output category (eg. Cat/Dog photos classifier), but instead of learning to classify pictures, we're going to learn (\"clone\") behaviours (as a sequence of decisions).\n",
    "\n",
    "We can think of this as a \"copy-cat\" of a given expert behaviour, from which we want to learn to imitate as close as possible.\n",
    "\n",
    "![](./img/terminology.png)\n",
    "\n",
    "The problem can be formalized as a probabilistic graphical model, where each node correspond to taking an action, being an state from which we only \"see\" an observation and \"moving\" to a new state.\n",
    "\n",
    "* **State:** True configuration of the system (eg. All physical variables of a ball falling)\n",
    "\n",
    "* **Observation:** Results from the state. Maybe NOT contain the whole information of the state (eg. A photo of the ball falling)\n",
    "\n",
    "![](./img/observation_and_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/imitation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only one photo (per timestep) as input **DOES NOT work** right away.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Small mistakes get accumulated until the observed state is very different from the demostrator behaviour, resulting in very bad performance (because the states are unknown for the agent).\n",
    "\n",
    "#### One solution\n",
    "\n",
    "Use 3 photos: a centered one, and a diagonally pointing to the left/right, each labeled as \"go forth\", \"rotate (compensate) right\" and \"rotate left\" \n",
    "\n",
    "![](./img/three_input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAgger: Dataset Aggregation\n",
    "\n",
    "![](./img/dagger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we're going to change step 3:\n",
    "\n",
    "> Instead of asking a human, we're going to load an expert trained neural network, and sample an action given an observation.\n",
    "\n",
    "This happens inside `rl_trainer.py` on the method `do_relabel_with_expert(...)`\n",
    "\n",
    "```python\n",
    "    def do_relabel_with_expert(self, expert_policy, paths):\n",
    "        print(\"Relabelling collected observations with labels from an expert policy...\")\n",
    "\n",
    "        # TODO relabel collected obsevations (from our policy) with labels from an expert policy\n",
    "        # HINT: query the policy (using the get_action function) with paths[i][\"observation\"]\n",
    "        # and replace paths[i][\"action\"] with these expert labels\n",
    "        for i in range(len(paths)):\n",
    "            obs = paths[i][\"observation\"]\n",
    "            expert_action = expert_policy.get_action(obs)\n",
    "            paths[i][\"action\"] = expert_action\n",
    "        return paths\n",
    "```\n",
    "\n",
    "Where the returned `paths` is a list of dictionaries containing the information for the trajectories (s,a,r,s') (see utils.py to find a Path object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "As with the `do_relabel_with_expert` method, lets go over each TODO piece of code.\n",
    "\n",
    "The files in order to read and to be edited are:\n",
    "\n",
    "1. `scripts/run_hw1.py` (read-only file)\n",
    "\n",
    "2. `infrastructure/rl_trainer.py`\n",
    "\n",
    "3. `agents/bc_agent.py` (read-only file)\n",
    "\n",
    "4. `policies/MLP_policy.py`\n",
    "\n",
    "5. `infrastructure/replay_buffer.py`\n",
    "\n",
    "6. `infrastructure/utils.py`\n",
    "\n",
    "7. `infrastructure/pytorch_util.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `scripts/run_hw1.py` (read-only file)\n",
    "\n",
    "You can read all possible parameters to get a better idea of the algorithms to be implemented.\n",
    "\n",
    "Also you can edit the main method so you don't need to write the parameters in the console, but directly run the file, eg:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    args = ['--expert_policy_file', '/home/user/CS_285-Deep_Reinforcement_Learning/hw1/cs285/policies/experts/Ant.pkl',\n",
    "            '--expert_data', '/home/user/CS_285-Deep_Reinforcement_Learning/hw1/cs285/expert_data/expert_data_Ant-v2.pkl',\n",
    "            '--env_name', 'Ant-v2',\n",
    "            '--exp_name', 'bc_ant',\n",
    "            '--ep_len', '5000',\n",
    "            '--eval_batch_size', '5000',\n",
    "            '--train_batch_size', '1000',\n",
    "            '--num_agent_train_steps_per_iter', '100',\n",
    "            '--no_gpu'\n",
    "            ]\n",
    "    main(args)\n",
    "```\n",
    "\n",
    "Later on you can automate an hyperparameter search algorithms that calls main in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `infrastructure/rl_trainer.py`\n",
    "\n",
    "**Note**: This one uses `utils.sample_trajectories` and `utils.sample_n_trajectories`, so maybe you can read them right away to better understand whats going on here.\n",
    "\n",
    "**Note:** Added tqdm() to train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m OrderedDict\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpkbar\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pytorch_util \u001b[34mas\u001b[39;49;00m ptu\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlogger\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Logger\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m utils\r\n",
      "\r\n",
      "\u001b[37m# how many rollouts to save as videos to tensorboard\u001b[39;49;00m\r\n",
      "MAX_NVIDEO = \u001b[34m2\u001b[39;49;00m\r\n",
      "MAX_VIDEO_LEN = \u001b[34m40\u001b[39;49;00m  \u001b[37m# we overwrite this in the code below\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mRL_Trainer\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\r\n",
      "\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "        \u001b[37m## INIT\u001b[39;49;00m\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Get params, create logger, create TF session\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.params = params\r\n",
      "        \u001b[36mself\u001b[39;49;00m.logger = Logger(\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mlogdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "        \u001b[37m# Set random seeds\u001b[39;49;00m\r\n",
      "        seed = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mseed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        np.random.seed(seed)\r\n",
      "        torch.manual_seed(seed)\r\n",
      "        ptu.init_gpu(\r\n",
      "            use_gpu=\u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mno_gpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            gpu_id=\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mwhich_gpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        )\r\n",
      "\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "        \u001b[37m## ENV\u001b[39;49;00m\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Make the gym environment\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.env = gym.make(\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33menv_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.env.seed(seed)\r\n",
      "\r\n",
      "        \u001b[37m# Maximum length for episodes\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] \u001b[35mor\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.env.spec.max_episode_steps\r\n",
      "        MAX_VIDEO_LEN = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m# Is this env continuous, or self.discrete?\u001b[39;49;00m\r\n",
      "        discrete = \u001b[36misinstance\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.env.action_space, gym.spaces.Discrete)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33magent_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdiscrete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = discrete\r\n",
      "\r\n",
      "        \u001b[37m# Observation and action sizes\u001b[39;49;00m\r\n",
      "        ob_dim = \u001b[36mself\u001b[39;49;00m.env.observation_space.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        ac_dim = \u001b[36mself\u001b[39;49;00m.env.action_space.n \u001b[34mif\u001b[39;49;00m discrete \u001b[34melse\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.env.action_space.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33magent_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mac_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ac_dim\r\n",
      "        \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33magent_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mob_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ob_dim\r\n",
      "\r\n",
      "        \u001b[37m# simulation timestep, will be used for video saving\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m \u001b[36mdir\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.env):\r\n",
      "            \u001b[36mself\u001b[39;49;00m.fps = \u001b[34m1\u001b[39;49;00m/\u001b[36mself\u001b[39;49;00m.env.model.opt.timestep\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.fps = \u001b[36mself\u001b[39;49;00m.env.env.metadata[\u001b[33m'\u001b[39;49;00m\u001b[33mvideo.frames_per_second\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "        \u001b[37m## AGENT\u001b[39;49;00m\r\n",
      "        \u001b[37m#############\u001b[39;49;00m\r\n",
      "\r\n",
      "        agent_class = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33magent_class\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.agent = agent_class(\u001b[36mself\u001b[39;49;00m.env, \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33magent_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mrun_training_loop\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, n_iter, collect_policy, eval_policy,\r\n",
      "                        initial_expertdata=\u001b[34mNone\u001b[39;49;00m, relabel_with_expert=\u001b[34mFalse\u001b[39;49;00m,\r\n",
      "                        start_relabel_with_expert=\u001b[34m1\u001b[39;49;00m, expert_policy=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        :param n_iter:  number of (dagger) iterations\u001b[39;49;00m\r\n",
      "\u001b[33m        :param collect_policy:\u001b[39;49;00m\r\n",
      "\u001b[33m        :param eval_policy:\u001b[39;49;00m\r\n",
      "\u001b[33m        :param initial_expertdata:\u001b[39;49;00m\r\n",
      "\u001b[33m        :param relabel_with_expert:  whether to perform dagger\u001b[39;49;00m\r\n",
      "\u001b[33m        :param start_relabel_with_expert: iteration at which to start relabel with expert\u001b[39;49;00m\r\n",
      "\u001b[33m        :param expert_policy:\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# init vars at beginning of training\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.total_envsteps = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_time = time.time()\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m itr \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(n_iter):\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m********** Iteration \u001b[39;49;00m\u001b[33m%i\u001b[39;49;00m\u001b[33m ************\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m%itr)\r\n",
      "\r\n",
      "            \u001b[37m# decide if videos should be rendered/logged at this iteration\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m itr % \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mvideo_log_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mvideo_log_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] != -\u001b[34m1\u001b[39;49;00m:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.log_video = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.log_video = \u001b[34mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# decide if metrics should be logged\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m itr % \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mscalar_log_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.log_metrics = \u001b[34mTrue\u001b[39;49;00m\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.log_metrics = \u001b[34mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# collect trajectories, to be used for training\u001b[39;49;00m\r\n",
      "            training_returns = \u001b[36mself\u001b[39;49;00m.collect_training_trajectories(\r\n",
      "                itr,\r\n",
      "                initial_expertdata,\r\n",
      "                collect_policy,\r\n",
      "                \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "            )  \u001b[37m# HW1: implement this function below\u001b[39;49;00m\r\n",
      "            paths, envsteps_this_batch, train_video_paths = training_returns\r\n",
      "            \u001b[36mself\u001b[39;49;00m.total_envsteps += envsteps_this_batch\r\n",
      "\r\n",
      "            \u001b[37m# relabel the collected obs with actions from a provided expert policy\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m relabel_with_expert \u001b[35mand\u001b[39;49;00m itr>=start_relabel_with_expert:\r\n",
      "                paths = \u001b[36mself\u001b[39;49;00m.do_relabel_with_expert(expert_policy, paths)  \u001b[37m# HW1: implement this function below\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# add collected data to replay buffer\u001b[39;49;00m\r\n",
      "            \u001b[36mself\u001b[39;49;00m.agent.add_to_replay_buffer(paths)\r\n",
      "\r\n",
      "            \u001b[37m# train agent (using sampled data from replay buffer)\u001b[39;49;00m\r\n",
      "            training_logs = \u001b[36mself\u001b[39;49;00m.train_agent()  \u001b[37m# HW1: implement this function below\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# log/save\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.log_video \u001b[35mor\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.log_metrics:\r\n",
      "                \u001b[37m# perform logging\u001b[39;49;00m\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mBeginning logging procedure...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                \u001b[36mself\u001b[39;49;00m.perform_logging(\r\n",
      "                    itr, paths, eval_policy, train_video_paths, training_logs)\r\n",
      "\r\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33msave_params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\r\n",
      "                    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving agent params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                    \u001b[36mself\u001b[39;49;00m.agent.save(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/policy_itr_\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mlogdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], itr))\r\n",
      "\r\n",
      "    \u001b[37m####################################\u001b[39;49;00m\r\n",
      "    \u001b[37m####################################\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcollect_training_trajectories\u001b[39;49;00m(\r\n",
      "            \u001b[36mself\u001b[39;49;00m,\r\n",
      "            itr,\r\n",
      "            load_initial_expertdata,\r\n",
      "            collect_policy,\r\n",
      "            batch_size,\r\n",
      "    ):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        :param itr:\u001b[39;49;00m\r\n",
      "\u001b[33m        :param load_initial_expertdata:  path to expert data pkl file\u001b[39;49;00m\r\n",
      "\u001b[33m        :param collect_policy:  the current policy using which we collect data\u001b[39;49;00m\r\n",
      "\u001b[33m        :param batch_size:  the number of transitions we collect\u001b[39;49;00m\r\n",
      "\u001b[33m        :return:\u001b[39;49;00m\r\n",
      "\u001b[33m            paths: a list trajectories\u001b[39;49;00m\r\n",
      "\u001b[33m            envsteps_this_batch: the sum over the numbers of environment steps in paths\u001b[39;49;00m\r\n",
      "\u001b[33m            train_video_paths: paths which also contain videos for visualization purposes\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# TODO decide whether to load training data or use the current policy to collect more data\u001b[39;49;00m\r\n",
      "        \u001b[37m# HINT: depending on if it's the first iteration or not, decide whether to either\u001b[39;49;00m\r\n",
      "                \u001b[37m# (1) load the data. In this case you can directly return as follows\u001b[39;49;00m\r\n",
      "                \u001b[37m# ``` return loaded_paths, 0, None ```\u001b[39;49;00m\r\n",
      "\r\n",
      "                \u001b[37m# (2) collect `self.params['batch_size']` transitions\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m itr == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            \u001b[37m# Load pickle (.pkl) of data\u001b[39;49;00m\r\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(load_initial_expertdata, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m handle:\r\n",
      "                loaded_paths = np.load(handle, allow_pickle=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "            paths = loaded_paths\r\n",
      "            envsteps_this_batch = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# TODO collect `batch_size` samples to be used for training\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT1: use sample_trajectories from utils\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT2: you want each of these collected rollouts to be of length self.params['ep_len']\u001b[39;49;00m\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCollecting data to be used for training...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "            paths, envsteps_this_batch = utils.sample_trajectories(\u001b[36mself\u001b[39;49;00m.env, policy=collect_policy,\r\n",
      "                                                                   min_timesteps_per_batch=batch_size,\r\n",
      "                                                                   max_path_length=\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                                                                   render=\u001b[34mFalse\u001b[39;49;00m, render_mode=(\u001b[33m'\u001b[39;49;00m\u001b[33mrgb_array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)) \u001b[37m# TODO\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# collect more rollouts with the same policy, to be saved as videos in tensorboard\u001b[39;49;00m\r\n",
      "        \u001b[37m# note: here, we collect MAX_NVIDEO rollouts, each of length MAX_VIDEO_LEN\u001b[39;49;00m\r\n",
      "        train_video_paths = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.log_video:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCollecting train rollouts to be used for saving videos...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[37m## TODO look in utils and implement sample_n_trajectories\u001b[39;49;00m\r\n",
      "            \u001b[37m# Using self.params['ep_len'] instead of MAX_VIDEO_LEN\u001b[39;49;00m\r\n",
      "            train_video_paths = utils.sample_n_trajectories(\u001b[36mself\u001b[39;49;00m.env, collect_policy, MAX_NVIDEO, \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m paths, envsteps_this_batch, train_video_paths\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_agent\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining agent using sampled data from replay buffer...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        all_logs = []\r\n",
      "        loop = tqdm(\u001b[36mrange\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_agent_train_steps_per_iter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "        \u001b[34mfor\u001b[39;49;00m train_step \u001b[35min\u001b[39;49;00m loop:\r\n",
      "            \u001b[37m# TODO sample some data from the data buffer\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT1: use the agent's sample function\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT2: how much data = self.params['train_batch_size']\u001b[39;49;00m\r\n",
      "            bs = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = \u001b[36mself\u001b[39;49;00m.agent.sample(bs)  \u001b[37m# TODO\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[37m# TODO use the sampled data to train an agent\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT: use the agent's train function\u001b[39;49;00m\r\n",
      "            \u001b[37m# HINT: keep the agent's training log for debugging\u001b[39;49;00m\r\n",
      "            train_log = \u001b[36mself\u001b[39;49;00m.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)  \u001b[37m# TODO\u001b[39;49;00m\r\n",
      "            all_logs.append(train_log)\r\n",
      "            loop.set_postfix(loss=all_logs[-\u001b[34m1\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining Loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m all_logs\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdo_relabel_with_expert\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, expert_policy, paths):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRelabelling collected observations with labels from an expert policy...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# TODO relabel collected obsevations (from our policy) with labels from an expert policy\u001b[39;49;00m\r\n",
      "        \u001b[37m# HINT: query the policy (using the get_action function) with paths[i][\"observation\"]\u001b[39;49;00m\r\n",
      "        \u001b[37m# and replace paths[i][\"action\"] with these expert labels\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(paths)):\r\n",
      "            obs = paths[i][\u001b[33m\"\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "            expert_action = expert_policy.get_action(obs)\r\n",
      "            paths[i][\u001b[33m\"\u001b[39;49;00m\u001b[33maction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = expert_action\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m paths\r\n",
      "\r\n",
      "    \u001b[37m####################################\u001b[39;49;00m\r\n",
      "    \u001b[37m####################################\u001b[39;49;00m\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mfrom_params_to_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\r\n",
      "        name = params[\u001b[33m'\u001b[39;49;00m\u001b[33mexp_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        params_to_use = [\u001b[33m'\u001b[39;49;00m\u001b[33mdo_dagger\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mnum_agent_train_steps_per_iter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                         \u001b[33m'\u001b[39;49;00m\u001b[33mn_iter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33meval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                         \u001b[33m'\u001b[39;49;00m\u001b[33mn_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m params.items():\r\n",
      "            \u001b[34mif\u001b[39;49;00m k \u001b[35min\u001b[39;49;00m params_to_use:\r\n",
      "                name += \u001b[33m\"\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + (k.replace(\u001b[33m\"\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))[:\u001b[34m3\u001b[39;49;00m] + \u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(v)\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m name\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mlogs_to_file\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, logs):\r\n",
      "        logdir = \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mlogdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        file_name = \u001b[36mself\u001b[39;49;00m.from_params_to_name(\u001b[36mself\u001b[39;49;00m.params)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving >>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, logdir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + file_name)\r\n",
      "        \u001b[37m#params_json_obj = json.dumps(self.params)\u001b[39;49;00m\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(logdir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/metrics_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m out:\r\n",
      "            str_dict = {\u001b[36mstr\u001b[39;49;00m(k):\u001b[36mstr\u001b[39;49;00m(v) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m logs.items()}\r\n",
      "            json.dump(str_dict, out, sort_keys=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(logdir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/params_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m out:\r\n",
      "            str_dict = {\u001b[36mstr\u001b[39;49;00m(k):\u001b[36mstr\u001b[39;49;00m(v) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.params.items()}\r\n",
      "            json.dump(str_dict, out, sort_keys=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mperform_logging\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, itr, paths, eval_policy, train_video_paths, training_logs):\r\n",
      "        \u001b[37m# collect eval trajectories, for logging\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCollecting data for eval...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        eval_paths, eval_envsteps_this_batch = utils.sample_trajectories(\u001b[36mself\u001b[39;49;00m.env, eval_policy, \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33meval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \u001b[36mself\u001b[39;49;00m.params[\u001b[33m'\u001b[39;49;00m\u001b[33mep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "        \u001b[37m# save eval rollouts as videos in tensorboard event file\u001b[39;49;00m\r\n",
      "        \u001b[37m# print(train_video_paths)\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.log_video \u001b[35mand\u001b[39;49;00m train_video_paths != \u001b[34mNone\u001b[39;49;00m:\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCollecting video rollouts eval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            eval_video_paths = utils.sample_n_trajectories(\u001b[36mself\u001b[39;49;00m.env, eval_policy, MAX_NVIDEO, MAX_VIDEO_LEN, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "            \u001b[37m#save train/eval videos\u001b[39;49;00m\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving train rollouts as videos...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[36mself\u001b[39;49;00m.logger.log_paths_as_videos(train_video_paths, itr, fps=\u001b[36mself\u001b[39;49;00m.fps, max_videos_to_save=MAX_NVIDEO,\r\n",
      "                                            video_title=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_rollouts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[36mself\u001b[39;49;00m.logger.log_paths_as_videos(eval_video_paths, itr, fps=\u001b[36mself\u001b[39;49;00m.fps,max_videos_to_save=MAX_NVIDEO,\r\n",
      "                                             video_title=\u001b[33m'\u001b[39;49;00m\u001b[33meval_rollouts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# save eval metrics\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.log_metrics:\r\n",
      "            \u001b[37m# returns, for logging\u001b[39;49;00m\r\n",
      "            train_returns = [path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].sum() \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths]\r\n",
      "            eval_returns = [eval_path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].sum() \u001b[34mfor\u001b[39;49;00m eval_path \u001b[35min\u001b[39;49;00m eval_paths]\r\n",
      "\r\n",
      "            \u001b[37m# episode lengths, for logging\u001b[39;49;00m\r\n",
      "            train_ep_lens = [\u001b[36mlen\u001b[39;49;00m(path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths]\r\n",
      "            eval_ep_lens = [\u001b[36mlen\u001b[39;49;00m(eval_path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \u001b[34mfor\u001b[39;49;00m eval_path \u001b[35min\u001b[39;49;00m eval_paths]\r\n",
      "\r\n",
      "            \u001b[37m# decide what to log\u001b[39;49;00m\r\n",
      "            logs = OrderedDict()\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_AverageReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(eval_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_StdReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.std(eval_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_MaxReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.max(eval_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_MinReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.min(eval_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_AverageEpLen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(eval_ep_lens)\r\n",
      "\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_AverageReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(train_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_StdReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.std(train_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_MaxReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.max(train_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_MinReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.min(train_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_AverageEpLen\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(train_ep_lens)\r\n",
      "\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain_EnvstepsSoFar\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.total_envsteps\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mTimeSinceStart\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = time.time() - \u001b[36mself\u001b[39;49;00m.start_time\r\n",
      "            last_log = training_logs[-\u001b[34m1\u001b[39;49;00m]  \u001b[37m# Only use the last log for now\u001b[39;49;00m\r\n",
      "            logs.update(last_log)\r\n",
      "\r\n",
      "\r\n",
      "            \u001b[34mif\u001b[39;49;00m itr == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.initial_return = np.mean(train_returns)\r\n",
      "            logs[\u001b[33m\"\u001b[39;49;00m\u001b[33mInitial_DataCollection_AverageReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.initial_return\r\n",
      "\r\n",
      "            \u001b[37m# perform the logging\u001b[39;49;00m\r\n",
      "            \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m logs.items():\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m : \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(key, value))\r\n",
      "                \u001b[36mself\u001b[39;49;00m.logger.log_scalar(value, key, itr)\r\n",
      "                \u001b[37m# TODO: call to log values for matplotlib or similar\u001b[39;49;00m\r\n",
      "                \u001b[36mself\u001b[39;49;00m.logs_to_file(logs)\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDone logging...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[36mself\u001b[39;49;00m.logger.flush()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/infrastructure/rl_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And inside `perform_logging`, added a call to a method to save recorded statistics to a file (to open later and compare runs).\n",
    "\n",
    "```python\n",
    "            # perform the logging\n",
    "            for key, value in logs.items():\n",
    "                print('{} : {}'.format(key, value))\n",
    "                self.logger.log_scalar(value, key, itr)\n",
    "                # TODO: call to log values for matplotlib or similar\n",
    "                self.logs_to_file(logs)\n",
    "            print('Done logging...')\n",
    "            self.logger.flush()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `infrastructure/utils.py`\n",
    "\n",
    "Here are defined the methods used before.\n",
    "\n",
    "They \"sample trajectories\" ie. simulate and record T trajectories $\\tau = [ (s_1, a_1, r_1, s'_1), (s_2, a_2, r_2, s'_2), \\dots , (s_T, a_T, r_T, s'_T) ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msample_trajectory\u001b[39;49;00m(env, policy, max_path_length, render=\u001b[34mFalse\u001b[39;49;00m, render_mode=(\u001b[33m'\u001b[39;49;00m\u001b[33mrgb_array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\n",
      "    \u001b[37m# initialize env for the beginning of a new rollout\u001b[39;49;00m\n",
      "    ob = env.reset() \u001b[37m#TODO # HINT: should be the output of resetting the env\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# init vars\u001b[39;49;00m\n",
      "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
      "    steps = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\n",
      "\n",
      "        \u001b[37m# render image of the simulated env\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m render:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mrgb_array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m render_mode:\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[36mhasattr\u001b[39;49;00m(env, \u001b[33m'\u001b[39;49;00m\u001b[33msim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "                    image_obs.append(env.sim.render(camera_name=\u001b[33m'\u001b[39;49;00m\u001b[33mtrack\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, height=\u001b[34m500\u001b[39;49;00m, width=\u001b[34m500\u001b[39;49;00m)[::-\u001b[34m1\u001b[39;49;00m])\n",
      "                \u001b[34melse\u001b[39;49;00m:\n",
      "                    image_obs.append(env.render(mode=render_mode))\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mhuman\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m render_mode:\n",
      "                env.render(mode=render_mode)\n",
      "                time.sleep(env.model.opt.timestep)\n",
      "\n",
      "        \u001b[37m# use the most recent ob to decide what to do\u001b[39;49;00m\n",
      "        obs.append(ob)\n",
      "        ac = policy.get_action(ob)  \u001b[37m# TODO # HINT: query the policy's get_action function\u001b[39;49;00m\n",
      "        ac = ac[\u001b[34m0\u001b[39;49;00m]\n",
      "        acs.append(ac)\n",
      "\n",
      "        \u001b[37m# take that action and record results\u001b[39;49;00m\n",
      "        ob, rew, done, _ = env.step(ac)\n",
      "\n",
      "        \u001b[37m# record result of taking that action\u001b[39;49;00m\n",
      "        steps += \u001b[34m1\u001b[39;49;00m\n",
      "        next_obs.append(ob)\n",
      "        rewards.append(rew)\n",
      "\n",
      "        \u001b[37m# TODO end the rollout if the rollout ended\u001b[39;49;00m\n",
      "        \u001b[37m# HINT: rollout can end due to done, or due to max_path_length\u001b[39;49;00m\n",
      "        rollout_done = done \u001b[35mor\u001b[39;49;00m steps == max_path_length  \u001b[37m# TODO # HINT: this is either 0 or 1\u001b[39;49;00m\n",
      "        terminals.append(rollout_done)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m rollout_done:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m Path(obs, image_obs, acs, rewards, next_obs, terminals)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msample_trajectories\u001b[39;49;00m(env, policy, min_timesteps_per_batch, max_path_length, render=\u001b[34mFalse\u001b[39;49;00m, render_mode=(\u001b[33m'\u001b[39;49;00m\u001b[33mrgb_array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Collect rollouts until we have collected min_timesteps_per_batch steps.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        TODO implement this function\u001b[39;49;00m\n",
      "\u001b[33m        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\u001b[39;49;00m\n",
      "\u001b[33m        Hint2: use get_pathlength to count the timesteps collected in each path\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    timesteps_this_batch = \u001b[34m0\u001b[39;49;00m\n",
      "    paths = []\n",
      "    \u001b[34mwhile\u001b[39;49;00m timesteps_this_batch < min_timesteps_per_batch:\n",
      "        \u001b[37m# TODO\u001b[39;49;00m\n",
      "        paths.append(sample_trajectory(env, policy, max_path_length, render, render_mode))\n",
      "        timesteps_this_batch += get_pathlength(paths[-\u001b[34m1\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m paths, timesteps_this_batch\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msample_n_trajectories\u001b[39;49;00m(env, policy, ntraj, max_path_length, render=\u001b[34mFalse\u001b[39;49;00m, render_mode=(\u001b[33m'\u001b[39;49;00m\u001b[33mrgb_array\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Collect ntraj rollouts.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        TODO implement this function\u001b[39;49;00m\n",
      "\u001b[33m        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    paths = []\n",
      "\n",
      "    \u001b[37m# TODO\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(ntraj):\n",
      "        paths.append(sample_trajectory(env, policy, max_path_length, render, render_mode))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m paths\u001b[37m#, ntraj*max_path_length\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mPath\u001b[39;49;00m(obs, image_obs, acs, rewards, next_obs, terminals):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Take info (separate arrays) from a single rollout\u001b[39;49;00m\n",
      "\u001b[33m        and return it in a single dictionary\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m image_obs != []:\n",
      "        image_obs = np.stack(image_obs, axis=\u001b[34m0\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : np.array(obs, dtype=np.float32),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mimage_obs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : np.array(image_obs, dtype=np.uint8),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : np.array(rewards, dtype=np.float32),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33maction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : np.array(acs, dtype=np.float32),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mnext_observation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: np.array(next_obs, dtype=np.float32),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mterminal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: np.array(terminals, dtype=np.float32)}\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_listofrollouts\u001b[39;49;00m(paths, concat_rew=\u001b[34mTrue\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Take a list of rollout dictionaries\u001b[39;49;00m\n",
      "\u001b[33m        and return separate arrays,\u001b[39;49;00m\n",
      "\u001b[33m        where each array is a concatenation of that array from across the rollouts\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    observations = np.concatenate([path[\u001b[33m\"\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths])\n",
      "    actions = np.concatenate([path[\u001b[33m\"\u001b[39;49;00m\u001b[33maction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths])\n",
      "    \u001b[34mif\u001b[39;49;00m concat_rew:\n",
      "        rewards = np.concatenate([path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths])\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        rewards = [path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths]\n",
      "    next_observations = np.concatenate([path[\u001b[33m\"\u001b[39;49;00m\u001b[33mnext_observation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths])\n",
      "    terminals = np.concatenate([path[\u001b[33m\"\u001b[39;49;00m\u001b[33mterminal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths])\n",
      "    \u001b[34mreturn\u001b[39;49;00m observations, actions, rewards, next_observations, terminals\n",
      "\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\u001b[37m############################################\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_pathlength\u001b[39;49;00m(path):\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(path[\u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/infrastructure/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `agents/bc_agent.py` (read-only file)\n",
    "\n",
    "BC (as in Behaviour Clonning) Agent is a class that represents the agent entity.\n",
    "\n",
    "It's initialized with an random initialized network (see MLPPolicySL) an empty replay buffer, to train using Supervised Learning on top of expert data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mreplay_buffer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ReplayBuffer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpolicies\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mMLP_policy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MLPPolicySL\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbase_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BaseAgent\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBCAgent\u001b[39;49;00m(BaseAgent):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, env, agent_params):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(BCAgent, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[37m# init vars\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.env = env\r\n",
      "        \u001b[36mself\u001b[39;49;00m.agent_params = agent_params\r\n",
      "\r\n",
      "        \u001b[37m# actor/policy\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.actor = MLPPolicySL(\r\n",
      "            \u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mac_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mob_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mn_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            discrete=\u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mdiscrete\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "            learning_rate=\u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        )\r\n",
      "\r\n",
      "        \u001b[37m# replay buffer\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.replay_buffer = ReplayBuffer(\u001b[36mself\u001b[39;49;00m.agent_params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_replay_buffer_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, ob_no, ac_na, re_n, next_ob_no, terminal_n):\r\n",
      "        \u001b[37m# training a BC agent refers to updating its actor using\u001b[39;49;00m\r\n",
      "        \u001b[37m# the given observations and corresponding action labels\u001b[39;49;00m\r\n",
      "        log = \u001b[36mself\u001b[39;49;00m.actor.update(ob_no, ac_na)  \u001b[37m# HW1: you will modify this\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m log\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32madd_to_replay_buffer\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, paths):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.replay_buffer.add_rollouts(paths)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msample\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batch_size):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.replay_buffer.sample_random_data(batch_size)  \u001b[37m# HW1: you will modify this\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msave\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, path):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.actor.save(path)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/agents/bc_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `policies/MLP_policy.py`\n",
    "\n",
    "Note the differences and similitudes between discrete and continuous policy.\n",
    "\n",
    "MLP: Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mabc\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Any\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m functional \u001b[34mas\u001b[39;49;00m F\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m optim\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m distributions\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pytorch_util \u001b[34mas\u001b[39;49;00m ptu\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpolicies\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbase_policy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasePolicy\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMLPPolicy\u001b[39;49;00m(BasePolicy, nn.Module, metaclass=abc.ABCMeta):\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\n",
      "                 ac_dim,\n",
      "                 ob_dim,\n",
      "                 n_layers,\n",
      "                 size,\n",
      "                 discrete=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                 learning_rate=\u001b[34m1e-4\u001b[39;49;00m,\n",
      "                 training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                 nn_baseline=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                 **kwargs\n",
      "                 ):\n",
      "        \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m(**kwargs)\n",
      "\n",
      "        \u001b[37m# init vars\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.ac_dim = ac_dim\n",
      "        \u001b[36mself\u001b[39;49;00m.ob_dim = ob_dim\n",
      "        \u001b[36mself\u001b[39;49;00m.n_layers = n_layers\n",
      "        \u001b[36mself\u001b[39;49;00m.discrete = discrete\n",
      "        \u001b[36mself\u001b[39;49;00m.size = size\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = learning_rate\n",
      "        \u001b[36mself\u001b[39;49;00m.training = training\n",
      "        \u001b[36mself\u001b[39;49;00m.nn_baseline = nn_baseline\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.discrete:\n",
      "            \u001b[36mself\u001b[39;49;00m.logits_na = ptu.build_mlp(\n",
      "                input_size=\u001b[36mself\u001b[39;49;00m.ob_dim,\n",
      "                output_size=\u001b[36mself\u001b[39;49;00m.ac_dim,\n",
      "                n_layers=\u001b[36mself\u001b[39;49;00m.n_layers,\n",
      "                size=\u001b[36mself\u001b[39;49;00m.size,\n",
      "            )\n",
      "            \u001b[36mself\u001b[39;49;00m.logits_na.to(ptu.device)\n",
      "            \u001b[36mself\u001b[39;49;00m.mean_net = \u001b[34mNone\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.logstd = \u001b[34mNone\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.optimizer = optim.Adam(\u001b[36mself\u001b[39;49;00m.logits_na.parameters(),\n",
      "                                        \u001b[36mself\u001b[39;49;00m.learning_rate)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.logits_na = \u001b[34mNone\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.mean_net = ptu.build_mlp(\n",
      "                input_size=\u001b[36mself\u001b[39;49;00m.ob_dim,\n",
      "                output_size=\u001b[36mself\u001b[39;49;00m.ac_dim,\n",
      "                n_layers=\u001b[36mself\u001b[39;49;00m.n_layers, size=\u001b[36mself\u001b[39;49;00m.size,\n",
      "            )\n",
      "            \u001b[36mself\u001b[39;49;00m.mean_net.to(ptu.device)\n",
      "            \u001b[36mself\u001b[39;49;00m.logstd = nn.Parameter(\n",
      "                torch.zeros(\u001b[36mself\u001b[39;49;00m.ac_dim, dtype=torch.float32, device=ptu.device)\n",
      "            )\n",
      "            \u001b[36mself\u001b[39;49;00m.logstd.to(ptu.device)\n",
      "            \u001b[36mself\u001b[39;49;00m.optimizer = optim.Adam(\n",
      "                itertools.chain([\u001b[36mself\u001b[39;49;00m.logstd], \u001b[36mself\u001b[39;49;00m.mean_net.parameters()),\n",
      "                \u001b[36mself\u001b[39;49;00m.learning_rate\n",
      "            )\n",
      "\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msave\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, filepath):\n",
      "        torch.save(\u001b[36mself\u001b[39;49;00m.state_dict(), filepath)\n",
      "\n",
      "    \u001b[37m##################################\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_action\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, obs: np.ndarray) -> np.ndarray:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(obs.shape) > \u001b[34m1\u001b[39;49;00m:\n",
      "            observation = obs\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            observation = obs[\u001b[34mNone\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[37m# TODO return the action that the policy prescribes\u001b[39;49;00m\n",
      "        \u001b[37m# raise NotImplementedError\u001b[39;49;00m\n",
      "        \u001b[37m#print('discrete:', self.discrete)\u001b[39;49;00m\n",
      "        \u001b[37m#print('observation:', observation)\u001b[39;49;00m\n",
      "        ac = \u001b[36mself\u001b[39;49;00m.mean_net(torch.from_numpy(observation).float().to(ptu.device))\n",
      "        \u001b[37m#print('mean_net:', ac)\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m ac.cpu().detach().numpy()\n",
      "\n",
      "\n",
      "    \u001b[37m# update/train this policy\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mupdate\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, observations, actions, **kwargs):\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# This function defines the forward pass of the network.\u001b[39;49;00m\n",
      "    \u001b[37m# You can return anything you want, but you should be able to differentiate\u001b[39;49;00m\n",
      "    \u001b[37m# through it. For example, you can return a torch.FloatTensor. You can also\u001b[39;49;00m\n",
      "    \u001b[37m# return more flexible objects, such as a\u001b[39;49;00m\n",
      "    \u001b[37m# `torch.distributions.Distribution` object. It's up to you!\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, observation: torch.FloatTensor) -> Any:\n",
      "        \u001b[37m# raise NotImplementedError\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.mean_net(torch.from_numpy(observation))\n",
      "\n",
      "\n",
      "\u001b[37m#####################################################\u001b[39;49;00m\n",
      "\u001b[37m#####################################################\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMLPPolicySL\u001b[39;49;00m(MLPPolicy):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, ac_dim, ob_dim, n_layers, size, **kwargs):\n",
      "        \u001b[36msuper\u001b[39;49;00m().\u001b[32m__init__\u001b[39;49;00m(ac_dim, ob_dim, n_layers, size, **kwargs)\n",
      "        \u001b[36mself\u001b[39;49;00m.loss = nn.MSELoss()\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mupdate\u001b[39;49;00m(\n",
      "            \u001b[36mself\u001b[39;49;00m, observations, actions,\n",
      "            adv_n=\u001b[34mNone\u001b[39;49;00m, acs_labels_na=\u001b[34mNone\u001b[39;49;00m, qvals=\u001b[34mNone\u001b[39;49;00m\n",
      "    ):\n",
      "        \u001b[37m# TODO: update the policy and return the loss\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.discrete:\n",
      "            y = \u001b[36mself\u001b[39;49;00m.logits_na(torch.from_numpy(observations).to(ptu.device))\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            y = \u001b[36mself\u001b[39;49;00m.mean_net(torch.from_numpy(observations).to(ptu.device))\n",
      "        loss = \u001b[36mself\u001b[39;49;00m.loss(y, torch.from_numpy(actions).to(ptu.device))  \u001b[37m# TODO\u001b[39;49;00m\n",
      "        \u001b[37m# print('loss: {}'.format(loss))\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        \u001b[36mself\u001b[39;49;00m.optimizer.step()\n",
      "\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\n",
      "            \u001b[37m# You can add extra logging information here, but keep this line\u001b[39;49;00m\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mTraining Loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: ptu.to_numpy(loss),\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/policies/MLP_policy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `infrastructure/replay_buffer.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcs285\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minfrastructure\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mReplayBuffer\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, max_size=\u001b[34m1000000\u001b[39;49;00m):\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.max_size = max_size\r\n",
      "\r\n",
      "        \u001b[37m# store each rollout\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.paths = []\r\n",
      "\r\n",
      "        \u001b[37m# store (concatenated) component arrays from each rollout\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.obs = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.acs = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.rews = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.next_obs = \u001b[34mNone\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.terminals = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.obs:\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.obs.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32madd_rollouts\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, paths, concat_rew=\u001b[34mTrue\u001b[39;49;00m):\r\n",
      "\r\n",
      "        \u001b[37m# add new rollouts into our list of rollouts\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m path \u001b[35min\u001b[39;49;00m paths:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.paths.append(path)\r\n",
      "\r\n",
      "        \u001b[37m# convert new rollouts into their component arrays, and append them onto\u001b[39;49;00m\r\n",
      "        \u001b[37m# our arrays\u001b[39;49;00m\r\n",
      "        observations, actions, rewards, next_observations, terminals = (\r\n",
      "            convert_listofrollouts(paths, concat_rew))\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.obs \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.obs = observations[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.acs = actions[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.rews = rewards[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.next_obs = next_observations[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.terminals = terminals[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.obs = np.concatenate([\u001b[36mself\u001b[39;49;00m.obs, observations])[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.acs = np.concatenate([\u001b[36mself\u001b[39;49;00m.acs, actions])[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[34mif\u001b[39;49;00m concat_rew:\r\n",
      "                \u001b[36mself\u001b[39;49;00m.rews = np.concatenate(\r\n",
      "                    [\u001b[36mself\u001b[39;49;00m.rews, rewards]\r\n",
      "                )[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(rewards, \u001b[36mlist\u001b[39;49;00m):\r\n",
      "                    \u001b[36mself\u001b[39;49;00m.rews += rewards\r\n",
      "                \u001b[34melse\u001b[39;49;00m:\r\n",
      "                    \u001b[36mself\u001b[39;49;00m.rews.append(rewards)\r\n",
      "                \u001b[36mself\u001b[39;49;00m.rews = \u001b[36mself\u001b[39;49;00m.rews[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.next_obs = np.concatenate(\r\n",
      "                [\u001b[36mself\u001b[39;49;00m.next_obs, next_observations]\r\n",
      "            )[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "            \u001b[36mself\u001b[39;49;00m.terminals = np.concatenate(\r\n",
      "                [\u001b[36mself\u001b[39;49;00m.terminals, terminals]\r\n",
      "            )[-\u001b[36mself\u001b[39;49;00m.max_size:]\r\n",
      "\r\n",
      "    \u001b[37m########################################\u001b[39;49;00m\r\n",
      "    \u001b[37m########################################\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msample_random_data\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batch_size):\r\n",
      "        \u001b[34massert\u001b[39;49;00m (\r\n",
      "                \u001b[36mself\u001b[39;49;00m.obs.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "                == \u001b[36mself\u001b[39;49;00m.acs.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "                == \u001b[36mself\u001b[39;49;00m.rews.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "                == \u001b[36mself\u001b[39;49;00m.next_obs.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "                == \u001b[36mself\u001b[39;49;00m.terminals.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        )\r\n",
      "\r\n",
      "        \u001b[37m## TODO return batch_size number of random entries from each of the 5 component arrays above\u001b[39;49;00m\r\n",
      "        \u001b[37m## HINT 1: use np.random.permutation to sample random indices\u001b[39;49;00m\r\n",
      "        \u001b[37m## HINT 2: return corresponding data points from each array (i.e., not different indices from each array)\u001b[39;49;00m\r\n",
      "        \u001b[37m## HINT 3: look at the sample_recent_data function below\u001b[39;49;00m\r\n",
      "        \u001b[37m# Shuffle and take 'batch_size' firsts\u001b[39;49;00m\r\n",
      "        random_idxs = np.random.permutation(\u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.obs))[:batch_size]\r\n",
      "        obs_sample = \u001b[36mself\u001b[39;49;00m.obs[random_idxs]\r\n",
      "        acs_sample = \u001b[36mself\u001b[39;49;00m.acs[random_idxs]\r\n",
      "        rews_sample = \u001b[36mself\u001b[39;49;00m.rews[random_idxs]\r\n",
      "        next_obs_sample = \u001b[36mself\u001b[39;49;00m.next_obs[random_idxs]\r\n",
      "        terminals_sample = \u001b[36mself\u001b[39;49;00m.terminals[random_idxs]\r\n",
      "\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m obs_sample, acs_sample, rews_sample, next_obs_sample, terminals_sample\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msample_recent_data\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batch_size=\u001b[34m1\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m (\r\n",
      "            \u001b[36mself\u001b[39;49;00m.obs[-batch_size:],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.acs[-batch_size:],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.rews[-batch_size:],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.next_obs[-batch_size:],\r\n",
      "            \u001b[36mself\u001b[39;49;00m.terminals[-batch_size:],\r\n",
      "        )\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/infrastructure/replay_buffer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `infrastructure/pytorch_util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Union\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nn\r\n",
      "\r\n",
      "Activation = Union[\u001b[36mstr\u001b[39;49;00m, nn.Module]\r\n",
      "\r\n",
      "\r\n",
      "_str_to_activation = {\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.ReLU(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.Tanh(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mleaky_relu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.LeakyReLU(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.Sigmoid(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mselu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.SELU(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33msoftplus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.Softplus(),\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33midentity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: nn.Identity(),\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_mlp\u001b[39;49;00m(\r\n",
      "        input_size: \u001b[36mint\u001b[39;49;00m,\r\n",
      "        output_size: \u001b[36mint\u001b[39;49;00m,\r\n",
      "        n_layers: \u001b[36mint\u001b[39;49;00m,\r\n",
      "        size: \u001b[36mint\u001b[39;49;00m,\r\n",
      "        activation: Activation = \u001b[33m'\u001b[39;49;00m\u001b[33mtanh\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        output_activation: Activation = \u001b[33m'\u001b[39;49;00m\u001b[33midentity\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      ") -> nn.Module:\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Builds a feedforward neural network\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        arguments:\u001b[39;49;00m\r\n",
      "\u001b[33m            n_layers: number of hidden layers\u001b[39;49;00m\r\n",
      "\u001b[33m            size: dimension of each hidden layer\u001b[39;49;00m\r\n",
      "\u001b[33m            activation: activation of each hidden layer\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m            input_size: size of the input layer\u001b[39;49;00m\r\n",
      "\u001b[33m            output_size: size of the output layer\u001b[39;49;00m\r\n",
      "\u001b[33m            output_activation: activation of the output layer\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        returns:\u001b[39;49;00m\r\n",
      "\u001b[33m            MLP (nn.Module)\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(activation, \u001b[36mstr\u001b[39;49;00m):\r\n",
      "        activation = _str_to_activation[activation]\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(output_activation, \u001b[36mstr\u001b[39;49;00m):\r\n",
      "        output_activation = _str_to_activation[output_activation]\r\n",
      "\r\n",
      "    \u001b[37m# TODO: return a MLP. This should be an instance of nn.Module\u001b[39;49;00m\r\n",
      "    \u001b[37m# Note: nn.Sequential is an instance of nn.Module.\u001b[39;49;00m\r\n",
      "    \u001b[37m# raise NotImplementedError\u001b[39;49;00m\r\n",
      "\r\n",
      "    nnActiv = activation\r\n",
      "    nnOutActiv = output_activation\r\n",
      "\r\n",
      "    modules = []\r\n",
      "    modules.append(nn.Linear(input_size, size))\r\n",
      "    modules.append(nnActiv)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(n_layers):\r\n",
      "        modules.append(nn.Linear(size, size))\r\n",
      "        modules.append(nnActiv)\r\n",
      "    modules.append(nn.Linear(size, output_size))\r\n",
      "    modules.append(nnOutActiv)\r\n",
      "\r\n",
      "    model = nn.Sequential(*modules)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "device = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minit_gpu\u001b[39;49;00m(use_gpu=\u001b[34mTrue\u001b[39;49;00m, gpu_id=\u001b[34m0\u001b[39;49;00m):\r\n",
      "    \u001b[34mglobal\u001b[39;49;00m device\r\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[35mand\u001b[39;49;00m use_gpu:\r\n",
      "        device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(gpu_id))\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing GPU id \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(gpu_id))\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU not detected. Defaulting to CPU.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mset_device\u001b[39;49;00m(gpu_id):\r\n",
      "    torch.cuda.set_device(gpu_id)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfrom_numpy\u001b[39;49;00m(*args, **kwargs):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.from_numpy(*args, **kwargs).float().to(device)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mto_numpy\u001b[39;49;00m(tensor):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tensor.to(\u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).detach().numpy()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/infrastructure/pytorch_util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: `run_statistics.py` \n",
    "\n",
    "This one make it easier to explore hyperparameters running several simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrun_hw1\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m main \u001b[34mas\u001b[39;49;00m run_experiment\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m listdir\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpath\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m isfile, join\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[37m# class stats():\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mplot_results\u001b[39;49;00m(logdirs, search_space):\r\n",
      "    all_dicts = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m logdir \u001b[35min\u001b[39;49;00m logdirs:\r\n",
      "        \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m listdir(logdir):\r\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(f)[:\u001b[34m7\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "                \u001b[37m# Metrics file\u001b[39;49;00m\r\n",
      "                \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(join(logdir, f)) \u001b[34mas\u001b[39;49;00m json_file:\r\n",
      "                    data = \u001b[36mdict\u001b[39;49;00m(json.load(json_file))\r\n",
      "                    all_dicts.append(data)\r\n",
      "\r\n",
      "    \u001b[37m# Merge everything in one dict with list as values\u001b[39;49;00m\r\n",
      "    results = {k: [dic[k] \u001b[34mfor\u001b[39;49;00m dic \u001b[35min\u001b[39;49;00m all_dicts] \u001b[34mfor\u001b[39;49;00m k \u001b[35min\u001b[39;49;00m all_dicts[\u001b[34m0\u001b[39;49;00m]}\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m metric, values \u001b[35min\u001b[39;49;00m results.items():\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m>>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metric, values)\r\n",
      "        \u001b[37m# From string to float\u001b[39;49;00m\r\n",
      "        values = [\u001b[36mfloat\u001b[39;49;00m(v) \u001b[34mfor\u001b[39;49;00m v \u001b[35min\u001b[39;49;00m values]\r\n",
      "        fig, (ax1, ax2) = plt.subplots(\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m)\r\n",
      "        fig.suptitle(\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(metric))\r\n",
      "        ax1.plot(search_space, values)\r\n",
      "        \r\n",
      "        ax1.plot(search_space, values,\r\n",
      "                 label=\u001b[33m\"\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m / \u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(np.mean(values), np.std(values)),\r\n",
      "                 marker=\u001b[33m\"\u001b[39;49;00m\u001b[33mo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        ax2.boxplot(values)\r\n",
      "        ax1.title.set_text(\u001b[33m\"\u001b[39;49;00m\u001b[33mn = \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mlen\u001b[39;49;00m(results[\u001b[33m\"\u001b[39;49;00m\u001b[33mEval_MaxReturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])))\r\n",
      "        ax1.legend(title=\u001b[33m\"\u001b[39;49;00m\u001b[33mMean / Std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "        logdir = logdirs[-\u001b[34m1\u001b[39;49;00m]\r\n",
      "        plt.savefig(logdir + \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + metric + \u001b[33m'\u001b[39;49;00m\u001b[33m_.png\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        plt.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    args = [\u001b[33m'\u001b[39;49;00m\u001b[33m--expert_policy_file\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m/home/jack/CS_285-Deep_Reinforcement_Learning/hw1/cs285/policies/experts/Ant.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--expert_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,        \u001b[33m'\u001b[39;49;00m\u001b[33m/home/jack/CS_285-Deep_Reinforcement_Learning/hw1/cs285/expert_data/expert_data_Ant-v2.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--env_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,           \u001b[33m'\u001b[39;49;00m\u001b[33mAnt-v2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--exp_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,           \u001b[33m'\u001b[39;49;00m\u001b[33mbc_ant\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--ep_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,             \u001b[33m'\u001b[39;49;00m\u001b[33m1000\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,    \u001b[33m'\u001b[39;49;00m\u001b[33m5000\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,   \u001b[33m'\u001b[39;49;00m\u001b[33m100\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--num_agent_train_steps_per_iter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1000\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[37m#'--do_dagger',\u001b[39;49;00m\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--n_iter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--no_gpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33m--video_log_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "            ]\r\n",
      "    \u001b[37m#grid_train_steps = ['1', '5', '10', '100', '1000', '5000', '50000']\u001b[39;49;00m\r\n",
      "    \u001b[37m#grid_n_iter = ['2', '10', '20', '40']\u001b[39;49;00m\r\n",
      "    \u001b[37m#seeds = [int(s) for s in np.linspace(0, 10000, 10)]\u001b[39;49;00m\r\n",
      "    \u001b[37m#grid_n_layers = ['1','2','3','5','10', '30']\u001b[39;49;00m\r\n",
      "    \u001b[37m# Run and save statistics for each param\u001b[39;49;00m\r\n",
      "    logdirs = []\r\n",
      "    \u001b[37m#for seed in seeds:\u001b[39;49;00m\r\n",
      "    \u001b[37m#for ts in grid_train_steps:\u001b[39;49;00m\r\n",
      "    \u001b[37m# for n_it in grid_n_iter:\u001b[39;49;00m\r\n",
      "    \u001b[37m#for n_it in grid_n_iter:\u001b[39;49;00m\r\n",
      "    \u001b[37m#for n_layers in grid_n_layers:\u001b[39;49;00m\r\n",
      "    num_samples = \u001b[34m30\u001b[39;49;00m\r\n",
      "    seeds = np.random.choice(\u001b[34m10000\u001b[39;49;00m, num_samples, replace=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m n \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(num_samples):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m>>>>>>>>>> Starting experiment with n=\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(n))\r\n",
      "        args += [\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(seeds[n])]\r\n",
      "        \u001b[37m#args += ['--num_agent_train_steps_per_iter', str(ts)]\u001b[39;49;00m\r\n",
      "        \u001b[37m#args += ['--n_iter', str(n_it)]\u001b[39;49;00m\r\n",
      "        \u001b[37m#args += ['--n_layers', str(n_layers)]\u001b[39;49;00m\r\n",
      "        logdir = run_experiment(args)\r\n",
      "        logdirs.append(logdir)\r\n",
      "\r\n",
      "    \u001b[37m# Create plots\u001b[39;49;00m\r\n",
      "\r\n",
      "    plot_results(logdirs, np.arange(num_samples))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize cs285/scripts/run_statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
